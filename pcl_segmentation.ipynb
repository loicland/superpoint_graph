{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Cloud Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims at coding a single step to process point cloud : It details each processing step from a raw point cloud to a semantic segmented point cloud. The method will take the point cloud in input and will output the corresponding segmented point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/superpoint/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 88 from C header, got 96 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import sys\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "sys.path.append('partition/cut-pursuit/src')\n",
    "sys.path.append('partition/ply_c')\n",
    "sys.path.append('partition')\n",
    "import libcp\n",
    "import libply_c\n",
    "from graphs import *\n",
    "from provider import *\n",
    "sys.path.append('./providers')\n",
    "from datasets import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from providers.datasets import HelixDataset\n",
    "from collections import defaultdict\n",
    "import h5py\n",
    "import os\n",
    "from plyfile import PlyData, PlyElement\n",
    "import open3d as o3d\n",
    "\n",
    "sys.path.append('learning')\n",
    "sys.path.append('partition')\n",
    "import spg\n",
    "import graphnet\n",
    "import pointnet\n",
    "import metrics\n",
    "import provider\n",
    "import s3dis_dataset\n",
    "import custom_dataset\n",
    "\n",
    "from visualisation import display_cloud\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Point Cloud Segmentation Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Partitioning Point Cloud***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subsubsection adapt the code used in partition/parition.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitionning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _partition(path_to_pcl, k_nn_geof = 45, k_nn_adj = 10, lambda_edge_weight = 1., reg_strength = 0.03, d_se_max = 0, voxel_width = 0.03, ver_batch = 0, overwrite = 0 ):\n",
    "        \"\"\" Large-scale Point Cloud Segmentation with Superpoint Graphs\n",
    "        Input : - path_to_pcl : path to the raw point cloud .ply file.\n",
    "                - k_nn_geof : number of neighbors for the geometric features, type=int\n",
    "                - k_nn_adj : adjacency structure for the minimal partition, type=int\n",
    "                - lambda_edge_weight : parameter determine the edge weight for minimal part, type=float\n",
    "                - reg_strength : regularization strength for the minimal partition, type=float\n",
    "                - d_se_max : max length of super edges, type=float\n",
    "                - voxel_width : voxel size when subsampling (in m), type=float\n",
    "                - ver_batch : Batch size for reading large files, 0 do disable batch loading, type=int\n",
    "                - overwrite : Wether to read existing files or overwrite them, type=int\n",
    "        \"\"\"\n",
    "        \n",
    "        root, file =  os.path.dirname(os.path.dirname(os.path.split(os.path.abspath(path_to_pcl))[0])), os.path.split(os.path.abspath(path_to_pcl))[1]\n",
    "        root = root + '/'\n",
    "        \n",
    "        helix_data = HelixDataset()\n",
    "        folder = helix_data.folders[0]\n",
    "        n_labels = len(helix_data.labels.keys())\n",
    "        \n",
    "        times = [0,0,0] #time for computing: features / partition / spg\n",
    "\n",
    "        if not os.path.isdir(root + \"clouds\"):\n",
    "            os.mkdir(root + \"clouds\")\n",
    "        if not os.path.isdir(root + \"features\"):\n",
    "            os.mkdir(root + \"features\")\n",
    "        if not os.path.isdir(root + \"superpoint_graphs\"):\n",
    "            os.mkdir(root + \"superpoint_graphs\")\n",
    "            \n",
    "        print(\"=================\\n   \"+ 'Start Partitioning {}'.format(file)+\"\\n=================\")\n",
    "\n",
    "        data_folder = root   + \"data/\"              + folder\n",
    "        cloud_folder  = root + \"clouds/\"            + folder\n",
    "        fea_folder  = root   + \"features/\"          + folder\n",
    "        spg_folder  = root   + \"superpoint_graphs/\" + folder\n",
    "        if not os.path.isdir(data_folder):\n",
    "            raise ValueError(\"%s does not exist\" % data_folder)\n",
    "\n",
    "        if not os.path.isdir(cloud_folder):\n",
    "            os.mkdir(cloud_folder)\n",
    "        if not os.path.isdir(fea_folder):\n",
    "            os.mkdir(fea_folder)\n",
    "        if not os.path.isdir(spg_folder):\n",
    "            os.mkdir(spg_folder)\n",
    "\n",
    "        if not os.path.isfile(data_folder + file):\n",
    "            raise ValueError('{} does not exist in {}'.format(file, data_folder))\n",
    "\n",
    "        file_name   = os.path.splitext(os.path.basename(file))[0]\n",
    "\n",
    "        data_file   = data_folder      + file_name + helix_data.extension\n",
    "        cloud_file  = cloud_folder     + file_name\n",
    "        fea_file    = fea_folder       + file_name + '.h5'\n",
    "        spg_file    = spg_folder       + file_name + '.h5'\n",
    "       \n",
    "        #--- build the geometric feature file h5 file ---\n",
    "        if os.path.isfile(fea_file) and not overwrite:\n",
    "            print(\"    reading the existing feature file...\")\n",
    "            geof, xyz, rgb, graph_nn, labels = read_features(fea_file)\n",
    "        else :\n",
    "            print(\"    creating the feature file...\")\n",
    "            #--- read the data files and compute the labels---\n",
    "            \n",
    "            xyz = helix_data.read_pointcloud(data_file).astype(dtype='float32')\n",
    "            if voxel_width > 0:\n",
    "                xyz = libply_c.prune(xyz, voxel_width, np.zeros(xyz.shape,dtype='u1'), np.array(1,dtype='u1'), 0)[0]\n",
    "            labels = []\n",
    "            rgb = []\n",
    "            \n",
    "            start = timer()\n",
    "            #---compute 10 nn graph-------\n",
    "            graph_nn, target_fea = compute_graph_nn_2(xyz, k_nn_adj, k_nn_geof)\n",
    "            #---compute geometric features-------\n",
    "            geof = libply_c.compute_geof(xyz, target_fea, k_nn_geof).astype('float32')\n",
    "            end = timer()\n",
    "            times[0] = times[0] + end - start\n",
    "            del target_fea\n",
    "            write_features(fea_file, geof, xyz, rgb, graph_nn, labels)\n",
    "        #--compute the partition------\n",
    "        sys.stdout.flush()\n",
    "        if os.path.isfile(spg_file) and not overwrite:\n",
    "            print(\"    reading the existing superpoint graph file...\")\n",
    "            graph_sp, components, in_component = read_spg(spg_file)\n",
    "        else:\n",
    "            print(\"    computing the superpoint graph...\")\n",
    "            #--- build the spg h5 file --\n",
    "            features = geof\n",
    "            geof[:,3] = 2. * geof[:, 3]\n",
    "            \n",
    "            graph_nn[\"edge_weight\"] = np.array(1. / (lambda_edge_weight + graph_nn[\"distances\"] / np.mean(graph_nn[\"distances\"])), dtype = 'float32')\n",
    "            print(\"        minimal partition...\")\n",
    "            components, in_component = libcp.cutpursuit(features, graph_nn[\"source\"], graph_nn[\"target\"]\n",
    "                                         , graph_nn[\"edge_weight\"], reg_strength)\n",
    "            components = np.array(components, dtype = 'object')\n",
    "            end = timer()\n",
    "            times[1] = times[1] + end - start\n",
    "            print(\"        computation of the SPG...\")\n",
    "            start = timer()\n",
    "            graph_sp = compute_sp_graph(xyz, d_se_max, in_component, components, labels, n_labels)\n",
    "            end = timer()\n",
    "            times[2] = times[2] + end - start\n",
    "            write_spg(spg_file, graph_sp, components, in_component)\n",
    "\n",
    "        print(\"Timer : %5.1f / %5.1f / %5.1f \" % (times[0], times[1], times[2]))\n",
    "        print(\"=================\\n   \"+ 'Ended Partitioning {}'.format(file)+\"\\n=================\")\n",
    "        return root[:-1], folder + file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#root, file = _partition('data/TEST/data/test/room_1900.ply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "   Start Partitioning room_19065.ply\n",
      "=================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "room_19065.ply does not exist in /home/jovyan/superpoint_graph/data/TEST/data/test/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-32e16d6eebeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# normal if it fails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/TEST/data/test/room_19065.ply'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-6c4e7bfa8ebf>\u001b[0m in \u001b[0;36m_partition\u001b[0;34m(path_to_pcl, k_nn_geof, k_nn_adj, lambda_edge_weight, reg_strength, d_se_max, voxel_width, ver_batch, overwrite)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} does not exist in {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mfile_name\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: room_19065.ply does not exist in /home/jovyan/superpoint_graph/data/TEST/data/test/"
     ]
    }
   ],
   "source": [
    "# normal if it fails\n",
    "#_partition('data/TEST/data/test/room_19065.ply')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Embedding Semantic Informations***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model_path,model_config,db_info):\n",
    "    print(\"=> loading checkpoint '{}'\".format(model_path))\n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    checkpoint['args'].model_config = model_config \n",
    "    cloud_embedder = pointnet.CloudEmbedder(checkpoint['args'])\n",
    "    model = create_model(checkpoint['args'], db_info) #use original arguments, architecture can't change    \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    return model, cloud_embedder, checkpoint['args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(args, dbinfo):\n",
    "    \"\"\" Creates model \"\"\"\n",
    "    model = nn.Module()\n",
    "\n",
    "    nfeat = args.ptn_widths[1][-1]\n",
    "    model.ecc = graphnet.GraphNetwork(args.model_config, nfeat, [dbinfo['edge_feats']] + args.fnet_widths, args.fnet_orthoinit, args.fnet_llbias,args.fnet_bnidx, args.edge_mem_limit)\n",
    "\n",
    "    model.ptn = pointnet.PointNet(args.ptn_widths[0], args.ptn_widths[1], args.ptn_widths_stn[0], args.ptn_widths_stn[1], dbinfo['node_feats'], args.ptn_nfeat_stn, prelast_do=args.ptn_prelast_do)\n",
    "\n",
    "    print('Total number of parameters: {}'.format(sum([p.numel() for p in model.parameters()]))) \n",
    "    if args.cuda: \n",
    "        model.cuda()\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'results/s3dis/bw/cv1/model.pth.tar'\n",
      "Total number of parameters: 279025\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MODEL_PATH = 'results/s3dis/bw/cv1/model.pth.tar'\n",
    "model_config = 'gru_10_0,f_13'\n",
    "edge_attribs = 'delta_avg,delta_std,nlength/ld,surface/ld,volume/ld,size/ld,xyz/d'\n",
    "pc_attribs = 'xyzelspvXYZ'\n",
    "dbinfo = HelixDataset().get_info(edge_attribs,pc_attribs)\n",
    "\"\"\"\n",
    "#model,cloud_embedder, args = load_weights(MODEL_PATH,model_config,dbinfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(args, root):\n",
    "    args.ROOT_PATH = root\n",
    "    HelixDataset().preprocess_pointclouds(args.ROOT_PATH)\n",
    "    create_dataset = HelixDataset().get_datasets\n",
    "\n",
    "    collected = defaultdict(list)\n",
    "    for ss in range(args.test_multisamp_n):\n",
    "        eval_data = create_dataset(args,ss)[1]\n",
    "        loader = torch.utils.data.DataLoader(eval_data, batch_size=1, collate_fn=spg.eccpc_collate, num_workers=8)\n",
    "        for bidx, (targets, GIs, clouds_data) in enumerate(loader):\n",
    "            model.ecc.set_info(GIs, args.cuda)\n",
    "            label_mode_cpu, label_vec_cpu, segm_size_cpu = targets[:,0], targets[:,2:], targets[:,1:].sum(1).float()\n",
    "            data = clouds_data\n",
    "            embeddings = cloud_embedder.run(model, *clouds_data)\n",
    "            outputs = model.ecc(embeddings)\n",
    "            fname = clouds_data[0][0][:clouds_data[0][0].rfind('.')]\n",
    "            collected[fname].append((outputs.data.cpu().numpy(), label_mode_cpu.numpy(), label_vec_cpu.numpy()))\n",
    "    predictions = {}\n",
    "    with h5py.File(os.path.join(args.ROOT_PATH, 'predictions.h5'), 'w') as hf:\n",
    "        for fname,output in collected.items():\n",
    "            o_cpu, t_cpu, tvec_cpu = list(zip(*output))\n",
    "            o_cpu = np.mean(np.stack(o_cpu,0),0)\n",
    "            prediction = np.argmax(o_cpu,axis=-1)\n",
    "            predictions[fname] = prediction\n",
    "            hf.create_dataset(name=fname, data=prediction) #(0-based classes)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "room_1900.h5\n"
     ]
    }
   ],
   "source": [
    "#predictions = predict(args, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs Segmented Point Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(root_path, filename, predictions):\n",
    "    n_labels = 13\n",
    "    \n",
    "    folder = os.path.split(filename)[0] + '/'\n",
    "    file_name = os.path.split(filename)[1]\n",
    "    \n",
    "    #---load the values------------------------------------------------------------\n",
    "    fea_file   = os.path.join(root_path,'features',folder,file_name + '.h5')\n",
    "    spg_file   = os.path.join(root_path,'superpoint_graphs',folder,file_name + '.h5')\n",
    "    ply_folder = os.path.join(root_path,'clouds',folder)\n",
    "    ply_file   = os.path.join(ply_folder,file_name)\n",
    "\n",
    "    if not os.path.isdir(ply_folder ):\n",
    "        os.mkdir(ply_folder)\n",
    "    if (not os.path.isfile(fea_file)) :\n",
    "        raise ValueError(\"%s does not exist and is needed\" % fea_file)\n",
    "    \n",
    "    geof, xyz, rgb, graph_nn, labels = provider.read_features(fea_file)\n",
    "\n",
    "    if not os.path.isfile(spg_file):    \n",
    "        raise ValueError(\"%s does not exist and is needed to output the partition  or result ply\" % spg_file) \n",
    "    else:\n",
    "        graph_spg, components, in_component = provider.read_spg(spg_file)\n",
    "        \n",
    "    pred_red  = predictions[filename]        \n",
    "    if (len(pred_red) != len(components)):\n",
    "        raise ValueError(\"It looks like the spg is not adapted to the result file\") \n",
    "    pred_full = provider.reduced_labels2full(pred_red, components, len(xyz))\n",
    "    \n",
    "    print(\"writing the prediction file...\")\n",
    "    provider.prediction2ply(ply_file + \"_pred.ply\", xyz, pred_full+1, n_labels,  args.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing the prediction file...\n"
     ]
    }
   ],
   "source": [
    "#visualise(root, file, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Regrouping in a Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudSegmentation(object):\n",
    "    \"\"\"\n",
    "    Collection of functions used to segment a point cloud\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, MODEL_PATH, model_config, edge_attribs, pc_attribs, dataset):\n",
    "        self._model_path = MODEL_PATH\n",
    "        self._model_config = model_config\n",
    "        self._edge_attribs = edge_attribs\n",
    "        self._pc_attribs = pc_attribs\n",
    "        self._dataset = dataset\n",
    "        self._model = None\n",
    "        self._cloud_embedder = None\n",
    "        self._args = None\n",
    "        \n",
    "    \n",
    "    def process(self, input_pcl, save_model = False):\n",
    "        \"\"\" take a raw point cloud as input and output a segmented point cloud in a .ply file\"\"\"\n",
    "        self._args.dataset = dataset\n",
    "        root, folder, file = self._partition(input_pcl)\n",
    "        print(\"=================\\n   \"+ 'Running Inferences' +\"\\n=================\")\n",
    "        predictions = self._predict(root, folder, file)\n",
    "        xyz, xyz_labels = self._save(root, folder+file , predictions, save_model)\n",
    "        return xyz, xyz_labels\n",
    "    \n",
    "    \n",
    "    def display(self, xyz, xyz_labels):\n",
    "        n_labels = 13\n",
    "        clouds = []\n",
    "        labels = []\n",
    "        colors = []\n",
    "        \n",
    "        if self._dataset == 's3dis':\n",
    "            dataset_info = s3dis_dataset.get_info(self._edge_attribs,self._pc_attribs)\n",
    "        elif self._dataset == 'helix':\n",
    "            dataset_info = HelixDataset().get_info(self._edge_attribs,self._pc_attribs)\n",
    "        \n",
    "        for i_label in range(0, n_labels+1):\n",
    "            cloud = xyz[np.where(xyz_labels == i_label)]\n",
    "            # converting simple array to open3d.PointCloud object\n",
    "            pcd = o3d.PointCloud()\n",
    "            pcd.points = o3d.Vector3dVector(cloud)\n",
    "            if len(pcd.points) != 0 :\n",
    "                clouds.append(pcd)\n",
    "                labels.append(dataset_info['inv_class_map'][i_label])\n",
    "                colors.append(provider.get_color_from_label(i_label+1, dataset))\n",
    "        \n",
    "        colors = np.asarray(colors)/255\n",
    "        display_cloud(clouds = clouds, labels = labels, colors = colors)\n",
    "       \n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\" load the weiths of the model \"\"\"\n",
    "        print(\"=================\\n   \"+ 'Preparing Model' +\"\\n=================\")\n",
    "        print(\"=> loading checkpoint '{}'\".format(self._model_path))\n",
    "        checkpoint = torch.load(self._model_path)\n",
    "\n",
    "        checkpoint['args'].model_config = self._model_config \n",
    "        cloud_embedder = pointnet.CloudEmbedder(checkpoint['args'])\n",
    "        dbinfo = HelixDataset().get_info(self._edge_attribs,self._pc_attribs)\n",
    "        model = self._create_model(checkpoint['args'], dbinfo) #use original arguments, architecture can't change    \n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        self._model = model\n",
    "        self._cloud_embedder = cloud_embedder\n",
    "        self._args = checkpoint['args']\n",
    "        return \n",
    "    \n",
    "    \n",
    "    def load_prediction(self, root_path, filename, prediction_file):\n",
    "        \"\"\" load the predictions from a file\n",
    "        root_path : relative path to the data folder (containing features, superpoint graph... folders)\n",
    "        filename : name of the file without the extension\n",
    "        prediction_file : name of the prediction file\n",
    "        \"\"\"\n",
    "        n_labels = 13\n",
    "\n",
    "        folder = os.path.split(filename)[0] + '/'\n",
    "        file_name = os.path.split(filename)[1]\n",
    "\n",
    "        #---load the values------------------------------------------------------------\n",
    "        fea_file   = os.path.join(root_path,'features',folder,file_name + '.h5')\n",
    "        spg_file   = os.path.join(root_path,'superpoint_graphs',folder,file_name + '.h5')\n",
    "        ply_folder = os.path.join(root_path,'clouds',folder)\n",
    "        ply_file   = os.path.join(ply_folder,file_name)\n",
    "        res_file   = os.path.join(root_path, prediction_file)\n",
    "\n",
    "        if not os.path.isdir(ply_folder ):\n",
    "            os.mkdir(ply_folder)\n",
    "        if (not os.path.isfile(fea_file)) :\n",
    "            raise ValueError(\"%s does not exist and is needed\" % fea_file)\n",
    "\n",
    "        geof, xyz, rgb, graph_nn, labels = provider.read_features(fea_file)\n",
    "\n",
    "        if not os.path.isfile(spg_file):    \n",
    "            raise ValueError(\"%s does not exist and is needed to output the partition  or result ply\" % spg_file) \n",
    "        else:\n",
    "            graph_spg, components, in_component = provider.read_spg(spg_file)\n",
    "\n",
    "        if not os.path.isfile(res_file):\n",
    "            raise ValueError(\"%s does not exist and is needed.\" % res_file) \n",
    "        try:\n",
    "            pred_red  = np.array(h5py.File(res_file, 'r').get(folder + file_name))        \n",
    "            if (len(pred_red) != len(components)):\n",
    "                raise ValueError(\"It looks like the spg is not adapted to the result file\") \n",
    "            pred_full = provider.reduced_labels2full(pred_red, components, len(xyz))\n",
    "        except OSError:\n",
    "            raise ValueError(\"%s does not exist in %s\" % (folder + file_name, res_file))\n",
    "        \n",
    "        return xyz, pred_full\n",
    "        \n",
    "        \n",
    "    def _create_model(self, args, dbinfo):\n",
    "        \"\"\" Creates the model \"\"\"\n",
    "        model = nn.Module()\n",
    "        nfeat = args.ptn_widths[1][-1]\n",
    "        model.ecc = graphnet.GraphNetwork(args.model_config, nfeat, [dbinfo['edge_feats']] + args.fnet_widths, args.fnet_orthoinit, args.fnet_llbias,args.fnet_bnidx, args.edge_mem_limit)\n",
    "        model.ptn = pointnet.PointNet(args.ptn_widths[0], args.ptn_widths[1], args.ptn_widths_stn[0], args.ptn_widths_stn[1], dbinfo['node_feats'], args.ptn_nfeat_stn, prelast_do=args.ptn_prelast_do)\n",
    "        print('Total number of parameters: {}'.format(sum([p.numel() for p in model.parameters()]))) \n",
    "        if args.cuda: \n",
    "            model.cuda()\n",
    "        return model \n",
    "    \n",
    "    \n",
    "    def _partition(self, path_to_pcl, k_nn_geof = 45, k_nn_adj = 10, lambda_edge_weight = 1., reg_strength = 0.03, d_se_max = 0, voxel_width = 0.03, ver_batch = 0, overwrite = 0 ):\n",
    "            \"\"\" Large-scale Point Cloud Segmentation with Superpoint Graphs\n",
    "            Input : - path_to_pcl : path to the raw point cloud .ply file.\n",
    "                    - k_nn_geof : number of neighbors for the geometric features, type=int\n",
    "                    - k_nn_adj : adjacency structure for the minimal partition, type=int\n",
    "                    - lambda_edge_weight : parameter determine the edge weight for minimal part, type=float\n",
    "                    - reg_strength : regularization strength for the minimal partition, type=float\n",
    "                    - d_se_max : max length of super edges, type=float\n",
    "                    - voxel_width : voxel size when subsampling (in m), type=float\n",
    "                    - ver_batch : Batch size for reading large files, 0 do disable batch loading, type=int\n",
    "                    - overwrite : Wether to read existing files or overwrite them, type=int\n",
    "            \"\"\"\n",
    "            \n",
    "            if self._dataset == 's3dis':\n",
    "                root, file =  os.path.dirname(os.path.dirname(os.path.dirname(os.path.split(os.path.abspath(path_to_pcl))[0]))), os.path.split(os.path.abspath(path_to_pcl))[1]\n",
    "                root = root + '/'\n",
    "            elif self._dataset == 'helix':\n",
    "                root, file =  os.path.dirname(os.path.dirname(os.path.split(os.path.abspath(path_to_pcl))[0])), os.path.split(os.path.abspath(path_to_pcl))[1]\n",
    "                root = root + '/'\n",
    "            \n",
    "            #list of subfolders to be processed\n",
    "            if self._dataset == 's3dis':\n",
    "                folder = os.path.split(os.path.dirname(os.path.dirname(path_to_pcl)))[1] + '/'\n",
    "                n_labels = 13\n",
    "            elif self._dataset == 'helix':\n",
    "                helix_data = HelixDataset()\n",
    "                folder = os.path.split(os.path.split(os.path.abspath(path_to_pcl))[0])[1] + '/'\n",
    "                n_labels = len(helix_data.labels.keys())\n",
    "            else:\n",
    "                raise ValueError('%s is an unknown data set' % dataset)\n",
    "\n",
    "            times = [0,0,0] #time for computing: features / partition / spg\n",
    "\n",
    "            if not os.path.isdir(root + \"clouds\"):\n",
    "                os.mkdir(root + \"clouds\")\n",
    "            if not os.path.isdir(root + \"features\"):\n",
    "                os.mkdir(root + \"features\")\n",
    "            if not os.path.isdir(root + \"superpoint_graphs\"):\n",
    "                os.mkdir(root + \"superpoint_graphs\")\n",
    "\n",
    "            print(\"=================\\n   \"+ 'Start Partitioning {}'.format(file)+\"\\n=================\")\n",
    "            \n",
    "            data_folder = root   + \"data/\"              + folder\n",
    "            cloud_folder  = root + \"clouds/\"            + folder\n",
    "            fea_folder  = root   + \"features/\"          + folder\n",
    "            spg_folder  = root   + \"superpoint_graphs/\" + folder\n",
    "            if not os.path.isdir(data_folder):\n",
    "                raise ValueError(\"%s does not exist\" % data_folder)\n",
    "\n",
    "            if not os.path.isdir(cloud_folder):\n",
    "                os.mkdir(cloud_folder)\n",
    "            if not os.path.isdir(fea_folder):\n",
    "                os.mkdir(fea_folder)\n",
    "            if not os.path.isdir(spg_folder):\n",
    "                os.mkdir(spg_folder)\n",
    "            \n",
    "            file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "           \n",
    "            if self._dataset == 's3dis':\n",
    "                if not os.path.isfile(data_folder +  file_name + '/' + file):\n",
    "                    raise ValueError('{} does not exist in {}'.format(file, data_folder +  file_name + '/'))\n",
    "            elif self._dataset == 'helix':\n",
    "                if not os.path.isfile(data_folder +  file):\n",
    "                    raise ValueError('{} does not exist in {}'.format(file, data_folder))\n",
    "            \n",
    "            if self._dataset =='s3dis':\n",
    "                data_file   = data_folder      + file_name + '/' + file_name + \".txt\"\n",
    "                cloud_file  = cloud_folder     + file_name\n",
    "                fea_file    = fea_folder       + file_name + '.h5'\n",
    "                spg_file    = spg_folder       + file_name + '.h5'\n",
    "            elif self._dataset =='helix':\n",
    "                data_file   = data_folder      + file_name + helix_data.extension\n",
    "                cloud_file  = cloud_folder     + file_name\n",
    "                fea_file    = fea_folder       + file_name + '.h5'\n",
    "                spg_file    = spg_folder       + file_name + '.h5'\n",
    "\n",
    "            #--- build the geometric feature file h5 file ---\n",
    "            if os.path.isfile(fea_file) and not overwrite:\n",
    "                print(\"    reading the existing feature file...\")\n",
    "                geof, xyz, rgb, graph_nn, labels = read_features(fea_file)\n",
    "            else :\n",
    "                print(\"    creating the feature file...\")\n",
    "                #--- read the data files and compute the labels---\n",
    "                \n",
    "                if self._dataset ==' s3dis':\n",
    "                    xyz, rgb, labels = read_s3dis_format(data_file)\n",
    "                    if voxel_width > 0:\n",
    "                        xyz, rgb, labels = libply_c.prune(xyz, voxel_width, rgb, labels, n_labels)\n",
    "                elif self._dataset == 'helix' :\n",
    "                    xyz = helix_data.read_pointcloud(data_file).astype(dtype='float32')\n",
    "                    if voxel_width > 0:\n",
    "                        xyz = libply_c.prune(xyz, voxel_width, np.zeros(xyz.shape,dtype='u1'), np.array(1,dtype='u1'), 0)[0]\n",
    "                    labels = []\n",
    "                    rgb = []\n",
    "\n",
    "                start = timer()\n",
    "                #---compute 10 nn graph-------\n",
    "                graph_nn, target_fea = compute_graph_nn_2(xyz, k_nn_adj, k_nn_geof)\n",
    "                #---compute geometric features-------\n",
    "                geof = libply_c.compute_geof(xyz, target_fea, k_nn_geof).astype('float32')\n",
    "                end = timer()\n",
    "                times[0] = times[0] + end - start\n",
    "                del target_fea\n",
    "                write_features(fea_file, geof, xyz, rgb, graph_nn, labels)\n",
    "            #--compute the partition------\n",
    "            sys.stdout.flush()\n",
    "            if os.path.isfile(spg_file) and not overwrite:\n",
    "                print(\"    reading the existing superpoint graph file...\")\n",
    "                graph_sp, components, in_component = read_spg(spg_file)\n",
    "            else:\n",
    "                print(\"    computing the superpoint graph...\")\n",
    "                #--- build the spg h5 file --\n",
    "                features = geof\n",
    "                geof[:,3] = 2. * geof[:, 3]\n",
    "                if self._dataset =='s3dis':\n",
    "                    features = np.hstack((geof, rgb/255.)).astype('float32')#add rgb as a feature for partitioning\n",
    "                    features[:,3] = 2. * features[:,3] #increase importance of verticality (heuristic)\n",
    "                elif self._dataset =='helix':\n",
    "                    features = geof\n",
    "                    geof[:,3] = 2. * geof[:, 3]\n",
    "\n",
    "                graph_nn[\"edge_weight\"] = np.array(1. / (lambda_edge_weight + graph_nn[\"distances\"] / np.mean(graph_nn[\"distances\"])), dtype = 'float32')\n",
    "                print(\"        minimal partition...\")\n",
    "                components, in_component = libcp.cutpursuit(features, graph_nn[\"source\"], graph_nn[\"target\"]\n",
    "                                             , graph_nn[\"edge_weight\"], reg_strength)\n",
    "                components = np.array(components, dtype = 'object')\n",
    "                end = timer()\n",
    "                times[1] = times[1] + end - start\n",
    "                print(\"        computation of the SPG...\")\n",
    "                start = timer()\n",
    "                graph_sp = compute_sp_graph(xyz, d_se_max, in_component, components, labels, n_labels)\n",
    "                end = timer()\n",
    "                times[2] = times[2] + end - start\n",
    "                write_spg(spg_file, graph_sp, components, in_component)\n",
    "            print(\"Timer : %5.1f / %5.1f / %5.1f \" % (times[0], times[1], times[2]))\n",
    "            return root[:-1], folder, file_name\n",
    "    \n",
    "    def _predict(self, root, folder, file):\n",
    "        self._args.ROOT_PATH = root\n",
    "        self._args.S3DIS_PATH = 'data/custom_S3DIS'\n",
    "        file_name = file+'.h5'\n",
    "        if self._dataset == 's3dis':\n",
    "            create_dataset = s3dis_dataset.get_datasets\n",
    "        elif self._dataset == 'helix':\n",
    "            HelixDataset().preprocess_pointclouds(self._args.ROOT_PATH, single_file = True, filename = file_name, folder = folder)\n",
    "            create_dataset = HelixDataset().get_datasets\n",
    "        \n",
    "        collected = defaultdict(list)\n",
    "        for ss in range(self._args.test_multisamp_n):\n",
    "            eval_data = create_dataset(self._args, ss, single_file = True, filename = file_name, folder_s = folder)[1]\n",
    "            loader = torch.utils.data.DataLoader(eval_data, batch_size=1, collate_fn=spg.eccpc_collate, num_workers=8)\n",
    "            for bidx, (targets, GIs, clouds_data) in enumerate(loader):\n",
    "                self._model.ecc.set_info(GIs, self._args.cuda)\n",
    "                label_mode_cpu, label_vec_cpu, segm_size_cpu = targets[:,0], targets[:,2:], targets[:,1:].sum(1).float()\n",
    "                data = clouds_data\n",
    "                embeddings = self._cloud_embedder.run(self._model, *clouds_data)\n",
    "                outputs = self._model.ecc(embeddings)\n",
    "                fname = clouds_data[0][0][:clouds_data[0][0].rfind('.')]\n",
    "                collected[fname].append((outputs.data.cpu().numpy(), label_mode_cpu.numpy(), label_vec_cpu.numpy()))\n",
    "        predictions = {}\n",
    "        with h5py.File(os.path.join(self._args.ROOT_PATH, os.path.splitext(os.path.basename(file))[0] +'_predictions.h5'), 'w') as hf:\n",
    "            for fname,output in collected.items():\n",
    "                o_cpu, t_cpu, tvec_cpu = list(zip(*output))\n",
    "                o_cpu = np.mean(np.stack(o_cpu,0),0)\n",
    "                prediction = np.argmax(o_cpu,axis=-1)\n",
    "                predictions[fname] = prediction\n",
    "                hf.create_dataset(name=fname, data=prediction) #(0-based classes)\n",
    "        return predictions  \n",
    "    \n",
    "    \n",
    "    def _save(self, root_path, filename,predictions, save_model):\n",
    "        n_labels = 13\n",
    "\n",
    "        folder = os.path.split(filename)[0] + '/'\n",
    "        file_name = os.path.split(filename)[1]\n",
    "\n",
    "        #---load the values------------------------------------------------------------\n",
    "        fea_file   = os.path.join(root_path,'features',folder,file_name + '.h5')\n",
    "        spg_file   = os.path.join(root_path,'superpoint_graphs',folder,file_name + '.h5')\n",
    "        ply_folder = os.path.join(root_path,'clouds',folder)\n",
    "        ply_file   = os.path.join(ply_folder,file_name)\n",
    "\n",
    "        if not os.path.isdir(ply_folder ):\n",
    "            os.mkdir(ply_folder)\n",
    "        if (not os.path.isfile(fea_file)) :\n",
    "            raise ValueError(\"%s does not exist and is needed\" % fea_file)\n",
    "\n",
    "        geof, xyz, rgb, graph_nn, labels = provider.read_features(fea_file)\n",
    "\n",
    "        if not os.path.isfile(spg_file):    \n",
    "            raise ValueError(\"%s does not exist and is needed to output the partition  or result ply\" % spg_file) \n",
    "        else:\n",
    "            graph_spg, components, in_component = provider.read_spg(spg_file)\n",
    "\n",
    "        pred_red  = predictions[filename]        \n",
    "        if (len(pred_red) != len(components)):\n",
    "            raise ValueError(\"It looks like the spg is not adapted to the result file\") \n",
    "        pred_full = provider.reduced_labels2full(pred_red, components, len(xyz))\n",
    "        \n",
    "        if save_model:\n",
    "            print(\"=================\\n   \"+ 'Saving Segmented Point Cloud' +\"\\n=================\")\n",
    "            print(\"writing the prediction file (i.e Semantic Segmented Point Cloud) in {}...\".format(ply_folder))\n",
    "            provider.prediction2ply(ply_file + \"_pred.ply\", xyz, pred_full+1, n_labels,  self._dataset)\n",
    "        \n",
    "        return xyz, pred_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **How to use the Class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'results/s3dis/bw/cv1_2/model.pth.tar'\n",
    "model_config = 'gru_10_0,f_13'\n",
    "edge_attribs = 'delta_avg,delta_std,nlength/ld,surface/ld,volume/ld,size/ld,xyz/d'\n",
    "#pc_attribs = 'xyzelspvXYZ'\n",
    "pc_attribs = 'xyzelspv'\n",
    "dataset = 'helix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PointCloudSegmentation(MODEL_PATH, model_config, edge_attribs, pc_attribs, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "   Preparing Model\n",
      "=================\n",
      "=> loading checkpoint 'results/s3dis/bw/cv1_2/model.pth.tar'\n",
      "Total number of parameters: 278641\n"
     ]
    }
   ],
   "source": [
    "model.load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment the Point Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "   Start Partitioning test_02.ply\n",
      "=================\n",
      "    creating the feature file...\n",
      "    computing the superpoint graph...\n",
      "        minimal partition...\n",
      "        computation of the SPG...\n",
      "Timer :   2.0 /  10.6 /  19.9 \n",
      "/home/jovyan/superpoint_graph/data/TEST\n",
      "=================\n",
      "   Running Inferences\n",
      "=================\n",
      "test_02.h5\n"
     ]
    }
   ],
   "source": [
    "xyz, xyz_labels = model.process('data/TEST/data/test/test_02.ply') #set save_model to True if you want to write out the segmented point cloud. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or reading an existing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz, xyz_labels = model.load_prediction('data/weWork', 'demo/helix_san_mateo_lvl2_03_clean', 'helix_san_mateo_lvl2_03_clean_predictions.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "launching server\n",
      "server launched, go to https://jupyterhub.helix.re/user/thomas/proxy/3221/cloud_viewer.html\n"
     ]
    }
   ],
   "source": [
    "model.display(xyz, xyz_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:superpoint]",
   "language": "python",
   "name": "conda-env-superpoint-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
