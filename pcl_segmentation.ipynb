{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Cloud Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims at coding a single step to process point cloud : It details each processing step from a raw point cloud to a semantic segmented point cloud. The method will take the point cloud in input and will output the corresponding segmented point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Point Cloud Segmentation Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***1.Partitioning Point Cloud***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subsubsection adapt the code used in partition/parition.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "from timeit import default_timer as timer\n",
    "sys.path.append('partition/cut-pursuit/src')\n",
    "sys.path.append('partition/ply_c')\n",
    "sys.path.append('partition')\n",
    "import libcp\n",
    "import libply_c\n",
    "from graphs import *\n",
    "from provider import *\n",
    "sys.path.append('./providers')\n",
    "from datasets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.Partitionning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _partition(path_to_pcl, k_nn_geof = 45, k_nn_adj = 10, lambda_edge_weight = 1., reg_strength = 0.03, d_se_max = 0, voxel_width = 0.03, ver_batch = 0, overwrite = 0 ):\n",
    "        \"\"\" Large-scale Point Cloud Segmentation with Superpoint Graphs\n",
    "        Input : - path_to_pcl : path to the raw point cloud .ply file.\n",
    "                - k_nn_geof : number of neighbors for the geometric features, type=int\n",
    "                - k_nn_adj : adjacency structure for the minimal partition, type=int\n",
    "                - lambda_edge_weight : parameter determine the edge weight for minimal part, type=float\n",
    "                - reg_strength : regularization strength for the minimal partition, type=float\n",
    "                - d_se_max : max length of super edges, type=float\n",
    "                - voxel_width : voxel size when subsampling (in m), type=float\n",
    "                - ver_batch : Batch size for reading large files, 0 do disable batch loading, type=int\n",
    "                - overwrite : Wether to read existing files or overwrite them, type=int\n",
    "        \"\"\"\n",
    "        \n",
    "        root, file =  os.path.dirname(os.path.dirname(os.path.split(os.path.abspath(path_to_pcl))[0])), os.path.split(os.path.abspath(path_to_pcl))[1]\n",
    "        root = root + '/'\n",
    "        \n",
    "        helix_data = HelixDataset()\n",
    "        folder = helix_data.folders[0]\n",
    "        n_labels = len(helix_data.labels.keys())\n",
    "        \n",
    "        times = [0,0,0] #time for computing: features / partition / spg\n",
    "\n",
    "        if not os.path.isdir(root + \"clouds\"):\n",
    "            os.mkdir(root + \"clouds\")\n",
    "        if not os.path.isdir(root + \"features\"):\n",
    "            os.mkdir(root + \"features\")\n",
    "        if not os.path.isdir(root + \"superpoint_graphs\"):\n",
    "            os.mkdir(root + \"superpoint_graphs\")\n",
    "            \n",
    "        print(\"=================\\n   \"+ 'Start Partitioning {}'.format(file)+\"\\n=================\")\n",
    "\n",
    "        data_folder = root   + \"data/\"              + folder\n",
    "        cloud_folder  = root + \"clouds/\"            + folder\n",
    "        fea_folder  = root   + \"features/\"          + folder\n",
    "        spg_folder  = root   + \"superpoint_graphs/\" + folder\n",
    "        if not os.path.isdir(data_folder):\n",
    "            raise ValueError(\"%s does not exist\" % data_folder)\n",
    "\n",
    "        if not os.path.isdir(cloud_folder):\n",
    "            os.mkdir(cloud_folder)\n",
    "        if not os.path.isdir(fea_folder):\n",
    "            os.mkdir(fea_folder)\n",
    "        if not os.path.isdir(spg_folder):\n",
    "            os.mkdir(spg_folder)\n",
    "\n",
    "        if not os.path.isfile(data_folder + file):\n",
    "            raise ValueError('{} does not exist in {}'.format(file, data_folder))\n",
    "\n",
    "        file_name   = os.path.splitext(os.path.basename(file))[0]\n",
    "\n",
    "        data_file   = data_folder      + file_name + helix_data.extension\n",
    "        cloud_file  = cloud_folder     + file_name\n",
    "        fea_file    = fea_folder       + file_name + '.h5'\n",
    "        spg_file    = spg_folder       + file_name + '.h5'\n",
    "       \n",
    "        #--- build the geometric feature file h5 file ---\n",
    "        if os.path.isfile(fea_file) and not overwrite:\n",
    "            print(\"    reading the existing feature file...\")\n",
    "            geof, xyz, rgb, graph_nn, labels = read_features(fea_file)\n",
    "        else :\n",
    "            print(\"    creating the feature file...\")\n",
    "            #--- read the data files and compute the labels---\n",
    "            \n",
    "            xyz = helix_data.read_pointcloud(data_file).astype(dtype='float32')\n",
    "            if voxel_width > 0:\n",
    "                xyz = libply_c.prune(xyz, voxel_width, np.zeros(xyz.shape,dtype='u1'), np.array(1,dtype='u1'), 0)[0]\n",
    "            labels = []\n",
    "            rgb = []\n",
    "            \n",
    "            start = timer()\n",
    "            #---compute 10 nn graph-------\n",
    "            graph_nn, target_fea = compute_graph_nn_2(xyz, k_nn_adj, k_nn_geof)\n",
    "            #---compute geometric features-------\n",
    "            geof = libply_c.compute_geof(xyz, target_fea, k_nn_geof).astype('float32')\n",
    "            end = timer()\n",
    "            times[0] = times[0] + end - start\n",
    "            del target_fea\n",
    "            write_features(fea_file, geof, xyz, rgb, graph_nn, labels)\n",
    "        #--compute the partition------\n",
    "        sys.stdout.flush()\n",
    "        if os.path.isfile(spg_file) and not overwrite:\n",
    "            print(\"    reading the existing superpoint graph file...\")\n",
    "            graph_sp, components, in_component = read_spg(spg_file)\n",
    "        else:\n",
    "            print(\"    computing the superpoint graph...\")\n",
    "            #--- build the spg h5 file --\n",
    "            features = geof\n",
    "            geof[:,3] = 2. * geof[:, 3]\n",
    "            \n",
    "            graph_nn[\"edge_weight\"] = np.array(1. / (lambda_edge_weight + graph_nn[\"distances\"] / np.mean(graph_nn[\"distances\"])), dtype = 'float32')\n",
    "            print(\"        minimal partition...\")\n",
    "            components, in_component = libcp.cutpursuit(features, graph_nn[\"source\"], graph_nn[\"target\"]\n",
    "                                         , graph_nn[\"edge_weight\"], reg_strength)\n",
    "            components = np.array(components, dtype = 'object')\n",
    "            end = timer()\n",
    "            times[1] = times[1] + end - start\n",
    "            print(\"        computation of the SPG...\")\n",
    "            start = timer()\n",
    "            graph_sp = compute_sp_graph(xyz, d_se_max, in_component, components, labels, n_labels)\n",
    "            end = timer()\n",
    "            times[2] = times[2] + end - start\n",
    "            write_spg(spg_file, graph_sp, components, in_component)\n",
    "\n",
    "        print(\"Timer : %5.1f / %5.1f / %5.1f \" % (times[0], times[1], times[2]))\n",
    "        print(\"=================\\n   \"+ 'Ended Partitioning {}'.format(file)+\"\\n=================\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "   Start Partitioning room_1900.ply\n",
      "=================\n",
      "    reading the existing feature file...\n",
      "    reading the existing superpoint graph file...\n",
      "Timer :   0.0 /   0.0 /   0.0 \n",
      "=================\n",
      "   Ended Partitioning room_1900.ply\n",
      "=================\n"
     ]
    }
   ],
   "source": [
    "_partition('data/TEST/data/test/room_1900.ply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================\n",
      "   Start Partitioning room_19065.ply\n",
      "=================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "room_19065.ply does not exist in /home/jovyan/superpoint_graph/data/TEST/data/test/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-332d85efb200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/TEST/data/test/room_19065.ply'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-90-4646bddc5914>\u001b[0m in \u001b[0;36m_partition\u001b[0;34m(path_to_pcl, k_nn_geof, k_nn_adj, lambda_edge_weight, reg_strength, d_se_max, voxel_width, ver_batch, overwrite)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} does not exist in {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mfile_name\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: room_19065.ply does not exist in /home/jovyan/superpoint_graph/data/TEST/data/test/"
     ]
    }
   ],
   "source": [
    "_partition('data/TEST/data/test/room_19065.ply')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***2.Embedding Semantic Informations***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from providers.datasets import HelixDataset\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from plyfile import PlyData, PlyElement\n",
    "import open3d as o3d\n",
    "\n",
    "sys.path.append('learning')\n",
    "sys.path.append('partition')\n",
    "import spg\n",
    "import graphnet\n",
    "import pointnet\n",
    "import metrics\n",
    "import provider\n",
    "import s3dis_dataset\n",
    "import custom_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.Loading model and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'results/s3dis/bw/cv1/model.pth.tar'\n",
    "model_config = 'gru_10_0,f_13'\n",
    "edge_attribs = 'delta_avg,delta_std,nlength/ld,surface/ld,volume/ld,size/ld,xyz/d'\n",
    "pc_attribs = 'xyzelspvXYZ'\n",
    "dbinfo = HelixDataset().get_info(edge_attribs,pc_attribs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model_path,model_config,db_info):\n",
    "    print(\"=> loading checkpoint '{}'\".format(model_path))\n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    checkpoint['args'].model_config = model_config \n",
    "    cloud_embedder = pointnet.CloudEmbedder(checkpoint['args'])\n",
    "    model = create_model(checkpoint['args'], db_info) #use original arguments, architecture can't change    \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    return model, cloud_embedder, checkpoint['args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(args, dbinfo):\n",
    "    \"\"\" Creates model \"\"\"\n",
    "    model = nn.Module()\n",
    "\n",
    "    nfeat = args.ptn_widths[1][-1]\n",
    "    model.ecc = graphnet.GraphNetwork(args.model_config, nfeat, [dbinfo['edge_feats']] + args.fnet_widths, args.fnet_orthoinit, args.fnet_llbias,args.fnet_bnidx, args.edge_mem_limit)\n",
    "\n",
    "    model.ptn = pointnet.PointNet(args.ptn_widths[0], args.ptn_widths[1], args.ptn_widths_stn[0], args.ptn_widths_stn[1], dbinfo['node_feats'], args.ptn_nfeat_stn, prelast_do=args.ptn_prelast_do)\n",
    "\n",
    "    print('Total number of parameters: {}'.format(sum([p.numel() for p in model.parameters()]))) \n",
    "    if args.cuda: \n",
    "        model.cuda()\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.Run Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(args,create_dataset):\n",
    "    collected = defaultdict(list)\n",
    "    for ss in range(args.test_multisamp_n):\n",
    "        eval_data = create_dataset(args,ss)[1]\n",
    "        loader = torch.utils.data.DataLoader(eval_data, batch_size=1, collate_fn=spg.eccpc_collate, num_workers=8)\n",
    "        for bidx, (targets, GIs, clouds_data) in enumerate(loader):\n",
    "            model.ecc.set_info(GIs, args.cuda)\n",
    "            label_mode_cpu, label_vec_cpu, segm_size_cpu = targets[:,0], targets[:,2:], targets[:,1:].sum(1).float()\n",
    "            data = clouds_data\n",
    "            embeddings = cloud_embedder.run(model, *clouds_data)\n",
    "            outputs = model.ecc(embeddings)\n",
    "            fname = clouds_data[0][0][:clouds_data[0][0].rfind('.')]\n",
    "            collected[fname].append((outputs.data.cpu().numpy(), label_mode_cpu.numpy(), label_vec_cpu.numpy()))\n",
    "\n",
    "\n",
    "    predictions = {}\n",
    "    with h5py.File(os.path.join(args.ROOT_PATH, 'predictions.h5'), 'w') as hf:\n",
    "        for fname,output in collected.items():\n",
    "            o_cpu, t_cpu, tvec_cpu = list(zip(*output))\n",
    "            o_cpu = np.mean(np.stack(o_cpu,0),0)\n",
    "            prediction = np.argmax(o_cpu,axis=-1)\n",
    "            predictions[fname] = prediction\n",
    "            hf.create_dataset(name=fname, data=prediction) #(0-based classes)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(args,create_dataset):\n",
    "    collected = defaultdict(list)\n",
    "    for ss in range(args.test_multisamp_n):\n",
    "        eval_data = create_dataset(args,ss)[1]\n",
    "        loader = torch.utils.data.DataLoader(eval_data, batch_size=1, collate_fn=spg.eccpc_collate, num_workers=8)\n",
    "        for bidx, (targets, GIs, clouds_data) in enumerate(loader):\n",
    "            model.ecc.set_info(GIs, args.cuda)\n",
    "            label_mode_cpu, label_vec_cpu, segm_size_cpu = targets[:,0], targets[:,2:], targets[:,1:].sum(1).float()\n",
    "            data = clouds_data\n",
    "            embeddings = cloud_embedder.run(model, *clouds_data)\n",
    "            outputs = model.ecc(embeddings)\n",
    "            fname = clouds_data[0][0][:clouds_data[0][0].rfind('.')]\n",
    "            collected[fname].append((outputs.data.cpu().numpy(), label_mode_cpu.numpy(), label_vec_cpu.numpy()))\n",
    "\n",
    "\n",
    "    predictions = {}\n",
    "    with h5py.File(os.path.join(args.ROOT_PATH, 'predictions.h5'), 'w') as hf:\n",
    "        for fname,output in collected.items():\n",
    "            o_cpu, t_cpu, tvec_cpu = list(zip(*output))\n",
    "            o_cpu = np.mean(np.stack(o_cpu,0),0)\n",
    "            prediction = np.argmax(o_cpu,axis=-1)\n",
    "            predictions[fname] = prediction\n",
    "            hf.create_dataset(name=fname, data=prediction) #(0-based classes)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(args,create_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.Outputs Segmented Point Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Regrouping in a Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudSegmentation(object):\n",
    "    \"\"\"\n",
    "    Collection of functions used to segment a point cloud\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, MODEL_PATH, model_config, edge_attribs, pc_attribs):\n",
    "        self._MODEL_PATH = MODEL_PATH\n",
    "        self._model_config = model_config\n",
    "        self._edge_attribs = edge_attribs\n",
    "        self._pc_attribs = pc_attribs\n",
    "        self._dbinfo = HelixDataset().get_info(edge_attribs,pc_attribs)\n",
    "        \n",
    "        \n",
    "    def _create_model(self, args, dbinfo):\n",
    "        \"\"\" Creates the model \"\"\"\n",
    "        model = nn.Module()\n",
    "        nfeat = args.ptn_widths[1][-1]\n",
    "        model.ecc = graphnet.GraphNetwork(args.model_config, nfeat, [dbinfo['edge_feats']] + args.fnet_widths, args.fnet_orthoinit, args.fnet_llbias,args.fnet_bnidx, args.edge_mem_limit)\n",
    "        model.ptn = pointnet.PointNet(args.ptn_widths[0], args.ptn_widths[1], args.ptn_widths_stn[0], args.ptn_widths_stn[1], dbinfo['node_feats'], args.ptn_nfeat_stn, prelast_do=args.ptn_prelast_do)\n",
    "        print('Total number of parameters: {}'.format(sum([p.numel() for p in model.parameters()]))) \n",
    "        if args.cuda: \n",
    "            model.cuda()\n",
    "        return model \n",
    "    \n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\" load the weiths of the model \"\"\"\n",
    "        print(\"=> loading checkpoint '{}'\".format(self._model_path))\n",
    "        checkpoint = torch.load(self._model_path)\n",
    "\n",
    "        checkpoint['args'].model_config = self._model_config \n",
    "        cloud_embedder = pointnet.CloudEmbedder(checkpoint['args'])\n",
    "        model = _create_model(checkpoint['args'], self._dbinfo) #use original arguments, architecture can't change    \n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        return model, cloud_embedder, checkpoint['args']\n",
    "    \n",
    "    \n",
    "    def _partition(path_to_pcl, k_nn_geof = 45, k_nn_adj = 10, lambda_edge_weight = 1., reg_strength = 0.03, d_se_max = 0, voxel_width = 0.03, ver_batch = 0, overwrite = 0 ):\n",
    "            \"\"\" Large-scale Point Cloud Segmentation with Superpoint Graphs\n",
    "            Input : - path_to_pcl : path to the raw point cloud .ply file.\n",
    "                    - k_nn_geof : number of neighbors for the geometric features, type=int\n",
    "                    - k_nn_adj : adjacency structure for the minimal partition, type=int\n",
    "                    - lambda_edge_weight : parameter determine the edge weight for minimal part, type=float\n",
    "                    - reg_strength : regularization strength for the minimal partition, type=float\n",
    "                    - d_se_max : max length of super edges, type=float\n",
    "                    - voxel_width : voxel size when subsampling (in m), type=float\n",
    "                    - ver_batch : Batch size for reading large files, 0 do disable batch loading, type=int\n",
    "                    - overwrite : Wether to read existing files or overwrite them, type=int\n",
    "            \"\"\"\n",
    "\n",
    "            root, file =  os.path.dirname(os.path.dirname(os.path.split(os.path.abspath(path_to_pcl))[0])), os.path.split(os.path.abspath(path_to_pcl))[1]\n",
    "            root = root + '/'\n",
    "\n",
    "            helix_data = HelixDataset()\n",
    "            folder = helix_data.folders[0]\n",
    "            n_labels = len(helix_data.labels.keys())\n",
    "\n",
    "            times = [0,0,0] #time for computing: features / partition / spg\n",
    "\n",
    "            if not os.path.isdir(root + \"clouds\"):\n",
    "                os.mkdir(root + \"clouds\")\n",
    "            if not os.path.isdir(root + \"features\"):\n",
    "                os.mkdir(root + \"features\")\n",
    "            if not os.path.isdir(root + \"superpoint_graphs\"):\n",
    "                os.mkdir(root + \"superpoint_graphs\")\n",
    "\n",
    "            print(\"=================\\n   \"+ 'Start Partitioning {}'.format(file)+\"\\n=================\")\n",
    "\n",
    "            data_folder = root   + \"data/\"              + folder\n",
    "            cloud_folder  = root + \"clouds/\"            + folder\n",
    "            fea_folder  = root   + \"features/\"          + folder\n",
    "            spg_folder  = root   + \"superpoint_graphs/\" + folder\n",
    "            if not os.path.isdir(data_folder):\n",
    "                raise ValueError(\"%s does not exist\" % data_folder)\n",
    "\n",
    "            if not os.path.isdir(cloud_folder):\n",
    "                os.mkdir(cloud_folder)\n",
    "            if not os.path.isdir(fea_folder):\n",
    "                os.mkdir(fea_folder)\n",
    "            if not os.path.isdir(spg_folder):\n",
    "                os.mkdir(spg_folder)\n",
    "\n",
    "            if not os.path.isfile(data_folder + file):\n",
    "                raise ValueError('{} does not exist in {}'.format(file, data_folder))\n",
    "\n",
    "            file_name   = os.path.splitext(os.path.basename(file))[0]\n",
    "\n",
    "            data_file   = data_folder      + file_name + helix_data.extension\n",
    "            cloud_file  = cloud_folder     + file_name\n",
    "            fea_file    = fea_folder       + file_name + '.h5'\n",
    "            spg_file    = spg_folder       + file_name + '.h5'\n",
    "\n",
    "            #--- build the geometric feature file h5 file ---\n",
    "            if os.path.isfile(fea_file) and not overwrite:\n",
    "                print(\"    reading the existing feature file...\")\n",
    "                geof, xyz, rgb, graph_nn, labels = read_features(fea_file)\n",
    "            else :\n",
    "                print(\"    creating the feature file...\")\n",
    "                #--- read the data files and compute the labels---\n",
    "\n",
    "                xyz = helix_data.read_pointcloud(data_file).astype(dtype='float32')\n",
    "                if voxel_width > 0:\n",
    "                    xyz = libply_c.prune(xyz, voxel_width, np.zeros(xyz.shape,dtype='u1'), np.array(1,dtype='u1'), 0)[0]\n",
    "                labels = []\n",
    "                rgb = []\n",
    "\n",
    "                start = timer()\n",
    "                #---compute 10 nn graph-------\n",
    "                graph_nn, target_fea = compute_graph_nn_2(xyz, k_nn_adj, k_nn_geof)\n",
    "                #---compute geometric features-------\n",
    "                geof = libply_c.compute_geof(xyz, target_fea, k_nn_geof).astype('float32')\n",
    "                end = timer()\n",
    "                times[0] = times[0] + end - start\n",
    "                del target_fea\n",
    "                write_features(fea_file, geof, xyz, rgb, graph_nn, labels)\n",
    "            #--compute the partition------\n",
    "            sys.stdout.flush()\n",
    "            if os.path.isfile(spg_file) and not overwrite:\n",
    "                print(\"    reading the existing superpoint graph file...\")\n",
    "                graph_sp, components, in_component = read_spg(spg_file)\n",
    "            else:\n",
    "                print(\"    computing the superpoint graph...\")\n",
    "                #--- build the spg h5 file --\n",
    "                features = geof\n",
    "                geof[:,3] = 2. * geof[:, 3]\n",
    "\n",
    "                graph_nn[\"edge_weight\"] = np.array(1. / (lambda_edge_weight + graph_nn[\"distances\"] / np.mean(graph_nn[\"distances\"])), dtype = 'float32')\n",
    "                print(\"        minimal partition...\")\n",
    "                components, in_component = libcp.cutpursuit(features, graph_nn[\"source\"], graph_nn[\"target\"]\n",
    "                                             , graph_nn[\"edge_weight\"], reg_strength)\n",
    "                components = np.array(components, dtype = 'object')\n",
    "                end = timer()\n",
    "                times[1] = times[1] + end - start\n",
    "                print(\"        computation of the SPG...\")\n",
    "                start = timer()\n",
    "                graph_sp = compute_sp_graph(xyz, d_se_max, in_component, components, labels, n_labels)\n",
    "                end = timer()\n",
    "                times[2] = times[2] + end - start\n",
    "                write_spg(spg_file, graph_sp, components, in_component)\n",
    "            print(\"Timer : %5.1f / %5.1f / %5.1f \" % (times[0], times[1], times[2]))\n",
    "            print(\"=================\\n   \"+ 'Ended Partitioning {}'.format(file)+\"\\n=================\")\n",
    "            return\n",
    "    \n",
    "    def _predict(self, arg, part_pcl):\n",
    "        \"\"\" run Inferences on the partitioned point cloud \"\"\"\n",
    "        return predictions\n",
    "        \n",
    "    \n",
    "    \n",
    "    def _visualize(self, predictions, output_filename):\n",
    "        \"\"\" output the results in a output_filenale.ply file \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def process(self, input_pcl, output_filename):\n",
    "        \"\"\" take a raw point cloud as input and output a segmented point cloud in a output_filename.ply file\"\"\"\n",
    "        model,cloud_embedder, args = _load_model()\n",
    "        part_pcl = _partition(input_pcl)\n",
    "        predictions = _predict(args,part_pcl)\n",
    "        _visualize(predictions, output_filename)\n",
    "        return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **How to use the Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:superpoint]",
   "language": "python",
   "name": "conda-env-superpoint-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
