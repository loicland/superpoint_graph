{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Superpoint inference\n",
    "This notebook uses a pretrained super point model to do inference on a new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/superpoint/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from providers.datasets import HelixDataset\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from plyfile import PlyData, PlyElement\n",
    "import open3d as o3d\n",
    "\n",
    "sys.path.append('learning')\n",
    "sys.path.append('partition')\n",
    "import spg\n",
    "import graphnet\n",
    "import pointnet\n",
    "import metrics\n",
    "import provider\n",
    "import s3dis_dataset\n",
    "import custom_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import model and load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'results/s3dis/bw/cv1/model.pth.tar'\n",
    "model_config = 'gru_10_0,f_13'\n",
    "edge_attribs = 'delta_avg,delta_std,nlength/ld,surface/ld,volume/ld,size/ld,xyz/d'\n",
    "pc_attribs = 'xyzelspvXYZ'\n",
    "dbinfo = HelixDataset().get_info(edge_attribs,pc_attribs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path,model_config,db_info):\n",
    "    print(\"=> loading checkpoint '{}'\".format(model_path))\n",
    "    checkpoint = torch.load(model_path)\n",
    "    \n",
    "    checkpoint['args'].model_config = model_config \n",
    "    cloud_embedder = pointnet.CloudEmbedder(checkpoint['args'])\n",
    "    model = create_model(checkpoint['args'], dbinfo) #use original arguments, architecture can't change    \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    return model, cloud_embedder, checkpoint['args']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(args, dbinfo):\n",
    "    \"\"\" Creates model \"\"\"\n",
    "    model = nn.Module()\n",
    "\n",
    "    nfeat = args.ptn_widths[1][-1]\n",
    "    model.ecc = graphnet.GraphNetwork(args.model_config, nfeat, [dbinfo['edge_feats']] + args.fnet_widths, args.fnet_orthoinit, args.fnet_llbias,args.fnet_bnidx, args.edge_mem_limit)\n",
    "\n",
    "    model.ptn = pointnet.PointNet(args.ptn_widths[0], args.ptn_widths[1], args.ptn_widths_stn[0], args.ptn_widths_stn[1], dbinfo['node_feats'], args.ptn_nfeat_stn, prelast_do=args.ptn_prelast_do)\n",
    "\n",
    "    print('Total number of parameters: {}'.format(sum([p.numel() for p in model.parameters()]))) \n",
    "    if args.cuda: \n",
    "        model.cuda()\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'results/s3dis/bw/cv1/model.pth.tar'\n",
      "Total number of parameters: 279025\n"
     ]
    }
   ],
   "source": [
    "model,cloud_embedder, args = load_model(MODEL_PATH,model_config,dbinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "room_1900.h5\n",
      "s3disconferenceRoom_1.h5\n",
      "99DuxtonRd.h5\n",
      "conference_room.h5\n",
      ".ipynb_checkpoints\n",
      "meeting_room_full.h5\n"
     ]
    }
   ],
   "source": [
    "data_to_test = 'helix'\n",
    "if data_to_test == 's3dis':\n",
    "    args.ROOT_PATH = 'data/S3DIS'\n",
    "    create_dataset = s3dis_dataset.get_datasets\n",
    "elif data_to_test == 'helix':\n",
    "    args.ROOT_PATH = 'data/helix'\n",
    "    args.S3DIS_PATH = 'data/S3DIS'\n",
    "    HelixDataset().preprocess_pointclouds(args.ROOT_PATH)\n",
    "    create_dataset = HelixDataset().get_datasets\n",
    "if data_to_test == 'custom':\n",
    "    args.ROOT_PATH = 'data/helix2'\n",
    "    create_dataset = custom_dataset.get_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected, predictions = defaultdict(list), {}\n",
    "for ss in range(args.test_multisamp_n):\n",
    "    eval_data = create_dataset(args,ss)[1]\n",
    "    loader = torch.utils.data.DataLoader(eval_data, batch_size=1, collate_fn=spg.eccpc_collate, num_workers=5)\n",
    "    for bidx, (targets, GIs, clouds_data) in enumerate(loader):\n",
    "        model.ecc.set_info(GIs, args.cuda)\n",
    "        label_mode_cpu, label_vec_cpu, segm_size_cpu = targets[:,0], targets[:,2:], targets[:,1:].sum(1).float()\n",
    "        data = clouds_data\n",
    "        embeddings = cloud_embedder.run(model, *clouds_data)\n",
    "        outputs = model.ecc(embeddings)\n",
    "        fname = clouds_data[0][0][:clouds_data[0][0].rfind('.')]\n",
    "        collected[fname].append((outputs.data.cpu().numpy(), label_mode_cpu.numpy(), label_vec_cpu.numpy()))\n",
    "\n",
    "\n",
    "with h5py.File(os.path.join(args.ROOT_PATH, 'predictions.h5'), 'w') as hf:\n",
    "    for fname,output in collected.items():\n",
    "        o_cpu, t_cpu, tvec_cpu = list(zip(*output))\n",
    "        o_cpu = np.mean(np.stack(o_cpu,0),0)\n",
    "        prediction = np.argmax(o_cpu,axis=-1)\n",
    "        predictions[fname] = prediction\n",
    "        hf.create_dataset(name=fname, data=prediction) #(0-based classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  4,  4,  9,  0,  2,  8,  2,  1,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8, 12, 12, 12,  8,  7, 12, 12,\n",
       "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  1,  1,  8,\n",
       "        8,  7,  2,  2,  0,  4,  4,  4,  4,  4,  4,  4,  4,  2,  2,  9,  9,\n",
       "        0,  0,  2,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions['Area_1/conferenceRoom_1'][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  6, 12,  0, 12,  2,  0,  9,  9, 12,  8,  9,  8,  8,  9,  9,  1,\n",
       "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8, 12, 12,  9,  8, 12, 12, 12,\n",
       "       12,  8, 12,  8, 12, 12, 12,  8, 12,  9,  9, 12, 12,  9, 12, 12, 12,\n",
       "       12, 12, 12, 12, 12,  8,  8, 12,  8,  9,  8,  8, 12,  8,  8,  8,  8,\n",
       "        7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8, 12,  8,\n",
       "        8, 12, 12, 12, 12,  8, 12, 12, 12,  9, 12, 12, 12, 12, 12])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions['test/s3disconferenceRoom_1'][0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "Parameters for visualisation:\n",
    "* i = input rgb pointcloud\n",
    "* g = ground truth,\n",
    "* f = geometric features, \n",
    "* p = partition, \n",
    "* r = prediction result\n",
    "* e = error\n",
    "* s = SPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing the prediction file...\n"
     ]
    }
   ],
   "source": [
    "visualise(args.ROOT_PATH,'r','test/s3disconferenceRoom_1',os.path.join(args.ROOT_PATH,'predictions.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(root_path, output_type,filename,prediction_file):\n",
    "\n",
    "    rgb_out = 'i' in output_type\n",
    "    gt_out  = 'g' in output_type\n",
    "    fea_out = 'f' in output_type\n",
    "    par_out = 'p' in output_type\n",
    "    res_out = 'r' in output_type\n",
    "    err_out = 'e' in output_type\n",
    "    spg_out = 's' in output_type\n",
    "    \n",
    "    n_labels = 13\n",
    "    \n",
    "    folder = os.path.split(filename)[0] + '/'\n",
    "    file_name = os.path.split(filename)[1]\n",
    "    \n",
    "    #---load the values------------------------------------------------------------\n",
    "    fea_file   = os.path.join(root_path,'features',folder,file_name + '.h5')\n",
    "    spg_file   = os.path.join(root_path,'superpoint_graphs',folder,file_name + '.h5')\n",
    "    ply_folder = os.path.join(root_path,'clouds',folder)\n",
    "    ply_file   = os.path.join(ply_folder,file_name)\n",
    "    res_file   = prediction_file\n",
    "\n",
    "    if not os.path.isdir(ply_folder ):\n",
    "        os.mkdir(ply_folder)\n",
    "    if (not os.path.isfile(fea_file)) :\n",
    "        raise ValueError(\"%s does not exist and is needed\" % fea_file)\n",
    "    \n",
    "    geof, xyz, rgb, graph_nn, labels = provider.read_features(fea_file)\n",
    "\n",
    "    if (par_out or res_out) and (not os.path.isfile(spg_file)):    \n",
    "        raise ValueError(\"%s does not exist and is needed to output the partition  or result ply\" % spg_file) \n",
    "    else:\n",
    "        graph_spg, components, in_component = provider.read_spg(spg_file)\n",
    "        \n",
    "    if res_out or err_out:\n",
    "        if not os.path.isfile(res_file):\n",
    "            raise ValueError(\"%s does not exist and is needed to output the result ply\" % res_file) \n",
    "        try:\n",
    "            pred_red  = np.array(h5py.File(res_file, 'r').get(folder + file_name))        \n",
    "            if (len(pred_red) != len(components)):\n",
    "                raise ValueError(\"It looks like the spg is not adapted to the result file\") \n",
    "            pred_full = provider.reduced_labels2full(pred_red, components, len(xyz))\n",
    "        except OSError:\n",
    "            raise ValueError(\"%s does not exist in %s\" % (folder + file_name, res_file))\n",
    "\n",
    "            #---write the output clouds----------------------------------------------------\n",
    "    if rgb_out:\n",
    "        print(\"writing the RGB file...\")\n",
    "        provider.write_ply(ply_file + \"_rgb.ply\", xyz, rgb)\n",
    "\n",
    "    if gt_out: \n",
    "        print(\"writing the GT file...\")\n",
    "        provider.prediction2ply(ply_file + \"_GT.ply\", xyz, labels, n_labels, args.dataset)\n",
    "\n",
    "    if fea_out:\n",
    "        print(\"writing the features file...\")\n",
    "        provider.geof2ply(ply_file + \"_geof.ply\", xyz, geof)\n",
    "\n",
    "    if par_out:\n",
    "        print(\"writing the partition file...\")\n",
    "        provider.partition2ply(ply_file + \"_partition.ply\", xyz, components)\n",
    "\n",
    "    if res_out:\n",
    "        print(\"writing the prediction file...\")\n",
    "        provider.prediction2ply(ply_file + \"_pred.ply\", xyz, pred_full+1, n_labels,  args.dataset)\n",
    "\n",
    "    if err_out:\n",
    "        print(\"writing the error file...\")\n",
    "        provider.error2ply(ply_file + \"_err.ply\", xyz, rgb, labels, pred_full+1)\n",
    "\n",
    "    if spg_out:\n",
    "        print(\"writing the SPG file...\")\n",
    "        provider.spg2ply(ply_file + \"_spg.ply\", graph_spg)\n",
    "\n",
    "#     if res_out and bool(args.upsample):\n",
    "#         if args.dataset=='s3dis':\n",
    "#             data_file   = root + 'data/' + folder + file_name + '/' + file_name + \".txt\"\n",
    "#             xyz_up, rgb_up = read_s3dis_format(data_file, False)\n",
    "#         elif args.dataset=='sema3d':#really not recommended unless you are very confident in your hardware\n",
    "#             data_file  = data_folder + file_name + \".txt\"\n",
    "#             xyz_up, rgb_up = read_semantic3d_format(data_file, 0, '', 0, args.ver_batch)\n",
    "#         elif args.dataset=='custom_dataset':\n",
    "#             data_file  = data_folder + file_name + \".ply\"\n",
    "#             xyz_up, rgb_up = read_ply(data_file)\n",
    "#         del rgb_up\n",
    "#         pred_up = interpolate_labels(xyz_up, xyz, pred_full, args.ver_batch)\n",
    "#         print(\"writing the upsampled prediction file...\")\n",
    "#         prediction2ply(ply_file + \"_pred_up.ply\", xyz_up, pred_up+1, n_labels, args.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:superpoint]",
   "language": "python",
   "name": "conda-env-superpoint-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
